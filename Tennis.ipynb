{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaboration and Competition\n",
    "\n",
    "---\n",
    "\n",
    "In this project, we develop two reinforcement learning agents to collaborate in a table tennis game, so as to keep the ball in the game as long as possible. If an agent hits the ball over the net, it receives a reward of +0.1. If an agent lets a ball hit the ground or hits the ball out of bounds, it receives a reward of -0.01. Thus, the goal of each agent is to keep the ball in play.\n",
    "\n",
    "### 0. Install dependencies\n",
    "\n",
    "Run the next code cell to install a few packages. This line will take a few minutes to run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install ./python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Setting environment variables\n",
    "\n",
    "Here we set env_name to the name of the Unity environment file we want to launch. We should ensure that the environment build is in the python directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = './python/Tennis_Windows_x86_64/Tennis.exe'  # Name of the Unity environment binary to launch\n",
    "train_mode = True                                       # Whether to run the environment in training or inference mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load dependencies\n",
    "\n",
    "The following loads the necessary dependencies and checks the Python version (at runtime). ML-Agents Toolkit (v0.3 onwards) requires Python 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version:\n",
      "3.6.7 |Anaconda, Inc.| (default, Oct 28 2018, 19:44:12) [MSC v.1915 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import random\n",
    "import copy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "from unityagents import UnityEnvironment\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"Python version:\")\n",
    "print(sys.version)\n",
    "\n",
    "# check Python version\n",
    "if (sys.version_info[0] < 3):\n",
    "    raise Exception(\"ERROR: ML-Agents Toolkit (v0.3 onwards) requires Python 3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Start the environment\n",
    "\n",
    "UnityEnvironment launches and begins communication with the environment when instantiated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=env_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Examine the State and Action Spaces\n",
    "\n",
    "We can reset the environment to be provided with an initial set of observations and states for all the agents within the environment. In ML-Agents, states refer to a vector of variables corresponding to relevant aspects of the environment for an agent. Likewise, observations refer to a set of relevant pixel-wise visuals for an agent.\n",
    "\n",
    "The observation space consists of 8 variables corresponding to the position and velocity of the ball and racket. Two continuous actions are available, corresponding to movement toward (or away from) the net, and jumping. \n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 2\n",
      "Size of each action: 2\n",
      "There are 2 agents. Each observes a state with length: 24\n",
      "The state for the first agent looks like: [ 0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.         -6.65278625 -1.5\n",
      " -0.          0.          6.83172083  6.         -0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents \n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Take Random Actions in the Environment\n",
    "\n",
    "Once we restart an environment, we can step the environment forward and provide actions to all of the agents within the environment. Here we simply choose random actions.\n",
    "\n",
    "Once this cell is executed, a message will be printed that detail how much reward was accumulated during one Episode. The Unity environment will then pause, waiting for further signals telling it what to do next. Thus, not seeing any animation is expected when running this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score (max over agents) from episode 1: 0.0\n",
      "Score (max over agents) from episode 2: 0.0\n",
      "Score (max over agents) from episode 3: 0.09000000171363354\n",
      "Score (max over agents) from episode 4: 0.10000000149011612\n",
      "Score (max over agents) from episode 5: 0.0\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 6):                                      # play game for 5 episodes\n",
    "    env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "    states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "    scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "    while True:\n",
    "        actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "        actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "        env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "        next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "        rewards = env_info.rewards                         # get reward (for each agent)\n",
    "        dones = env_info.local_done                        # see if episode finished\n",
    "        scores += env_info.rewards                         # update the score (for each agent)\n",
    "        states = next_states                               # roll over states to next time step\n",
    "        if np.any(dones):                                  # exit loop if episode finished\n",
    "            break\n",
    "    print('Score (max over agents) from episode {}: {}'.format(i, np.max(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Training the agents\n",
    "\n",
    "Now let us train our agents to solve the environment!\n",
    "\n",
    "#### 6.1 Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ddpg(n_episodes=2500, max_t=1000, print_every=100):\n",
    "    scores_deque = deque(maxlen=print_every)\n",
    "    score_list = []\n",
    "    avg_list = []\n",
    "    max_score = -np.Inf\n",
    "    \n",
    "    for e in range(1, n_episodes+1):\n",
    "        # Reset environment \n",
    "        env_info = env.reset(train_mode=True)[brain_name]\n",
    "        # Reset agent \n",
    "        agent.reset() \n",
    "        # Get the initial state \n",
    "        states = env_info.vector_observations      \n",
    "        # Initialize scores for the agents\n",
    "        scores = np.zeros(num_agents)\n",
    "        \n",
    "        for t in range(max_t):\n",
    "            # Get action\n",
    "            actions = agent.act(states)\n",
    "            # Observe reaction (environment)\n",
    "            env_info = env.step(actions)[brain_name]        \n",
    "            ## Get new state \n",
    "            next_states = env_info.vector_observations\n",
    "            # Get reward \n",
    "            rewards = env_info.rewards\n",
    "            # See if episode has finished \n",
    "            dones = env_info.local_done\n",
    "            # Step \n",
    "            agent.step(states, actions, rewards, next_states, dones)\n",
    "            \n",
    "            states = next_states\n",
    "            scores += rewards\n",
    "            \n",
    "            if np.any(dones):\n",
    "                break \n",
    "                \n",
    "        score = np.max(scores)\n",
    "        max_score = np.maximum(max_score, score)\n",
    "    \n",
    "        scores_deque.append(score)\n",
    "        score_list.append(score)\n",
    "        avg_list.append(np.mean(scores_deque))\n",
    "        \n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}\\tMax score: {:.3f}'.format(e, np.mean(scores_deque), max_score), end=\"\")\n",
    "        \n",
    "        if e % print_every == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(e, np.mean(scores_deque)))\n",
    "            torch.save(agent.actor_local.state_dict(), 'model_checkpoints/actor_checkpoint.pth')\n",
    "            for index, critic in enumerate(agent.get_critic()):\n",
    "                torch.save(critic.state_dict(), 'model_checkpoints/critic_checkpoint_0{0}.pth'.format(index))\n",
    "        if np.mean(scores_deque) >= .5:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(e-100, np.mean(scores_deque)))\n",
    "            torch.save(agent.actor_local.state_dict(), 'actor_solution.pth')\n",
    "            for index, critic in enumerate(agent.get_critic()):\n",
    "                torch.save(critic.state_dict(), 'critic_solution_{0}.pth'.format(index))\n",
    "            break\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: 0.00\tMax score: 0.000\n",
      "Episode 200\tAverage Score: 0.00\tMax score: 0.000\n",
      "Episode 300\tAverage Score: 0.00\tMax score: 0.000\n",
      "Episode 400\tAverage Score: 0.00\tMax score: 0.100\n",
      "Episode 500\tAverage Score: 0.02\tMax score: 0.100\n",
      "Episode 600\tAverage Score: 0.04\tMax score: 0.100\n",
      "Episode 700\tAverage Score: 0.01\tMax score: 0.100\n",
      "Episode 800\tAverage Score: 0.00\tMax score: 0.100\n",
      "Episode 900\tAverage Score: 0.01\tMax score: 0.100\n",
      "Episode 1000\tAverage Score: 0.04\tMax score: 0.100\n",
      "Episode 1100\tAverage Score: 0.04\tMax score: 0.200\n",
      "Episode 1200\tAverage Score: 0.03\tMax score: 0.200\n",
      "Episode 1300\tAverage Score: 0.02\tMax score: 0.200\n",
      "Episode 1400\tAverage Score: 0.04\tMax score: 0.200\n",
      "Episode 1500\tAverage Score: 0.05\tMax score: 0.200\n",
      "Episode 1600\tAverage Score: 0.06\tMax score: 0.800\n",
      "Episode 1700\tAverage Score: 0.07\tMax score: 0.800\n",
      "Episode 1800\tAverage Score: 0.51\tMax score: 2.700\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'solved' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-819917b29758>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# Run the algorithm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mddpg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-7-c9bba44098ec>\u001b[0m in \u001b[0;36mddpg\u001b[1;34m(n_episodes, max_t, print_every)\u001b[0m\n\u001b[0;32m     49\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcritic\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_critic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m                 \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcritic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'model_checkpoints/critic_checkpoint_0{0}.pth'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores_deque\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m.5\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0msolved\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores_deque\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m             \u001b[0msolved\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'solved' referenced before assignment"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from maddpg_agent import MADDPG\n",
    "\n",
    "# Initialize a DDPG Agent\n",
    "agent = MADDPG(state_size=state_size, action_size=action_size, n_agents=num_agents, random_seed=1)\n",
    "\n",
    "# Run the algorithm\n",
    "scores = ddpg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0\tScore: 0.0\n",
      "Episode: 1\tScore: 0.0\n",
      "Episode: 2\tScore: 0.0\n",
      "Episode: 3\tScore: 0.0\n",
      "Episode: 4\tScore: 0.0\n"
     ]
    }
   ],
   "source": [
    "# load the weights from file\n",
    "agent.actor_local.load_state_dict(torch.load('model_checkpoints/actor_checkpoint.pth'))\n",
    "\n",
    "for i in range(5):\n",
    "    env_info = env.reset(train_mode=False)[brain_name]  # reset the environment\n",
    "    states = env_info.vector_observations               # get the current state\n",
    "    scores = np.zeros(num_agents)                       # initialize the score\n",
    "    while True:\n",
    "        action = agent.act(states)                      # select an action\n",
    "        env_info = env.step(action)[brain_name]         # send the action to the environment\n",
    "        next_states = env_info.vector_observations      # get the next state\n",
    "        rewards = env_info.rewards                      # get the reward\n",
    "        dones = env_info.local_done                     # see if episode has finished\n",
    "        scores += rewards                               # update the score\n",
    "        state = next_states                             # roll over the state to next time step\n",
    "        if np.any(dones):                               # exit loop if episode finished\n",
    "            break\n",
    "\n",
    "    print(\"Episode: {0}\\tScore: {1}\".format(i, np.max(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(score_list)+1), score_list)\n",
    "plt.plot(np.arange(1, len(avg_list)+1), avg_list)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:drl_unity]",
   "language": "python",
   "name": "conda-env-drl_unity-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
